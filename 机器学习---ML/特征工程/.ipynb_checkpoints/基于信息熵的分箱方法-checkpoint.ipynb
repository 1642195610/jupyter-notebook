{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSplitByEntropy:\n",
    "    def __init__(self, group, threshold):\n",
    "        # 最大分组数\n",
    "        self.max_group = group\n",
    "        # 停止划分的最小熵\n",
    "        self.min_threshold = threshold\n",
    "        # 保存最终结果变量\n",
    "        self.result = dict()\n",
    "      \n",
    "    # 加载数据，可以改写成读取外源数据，并整理成（特征，标签）的形式\n",
    "    def load_data(self):\n",
    "        data = np.array(\n",
    "            [\n",
    "                [56, 1],   [87, 1],  [129, 0],  [23, 0],   [342, 1], \n",
    "                [641, 1],  [63, 0],  [2764, 1], [2323, 0], [453, 1], \n",
    "                [10, 1],   [9, 0],   [88, 1],   [222, 0],  [97, 0],\n",
    "                [2398, 1], [592, 1], [561, 1],  [764, 0],  [121, 1],\n",
    "                [28, 0],   [49, 1],  [361, 1],  [164, 0],  [1210, 1]\n",
    "            ]\n",
    "        )\n",
    "        return data\n",
    "\n",
    "    # 计算信息熵\n",
    "    def cal_entropy(self, data):\n",
    "        num_data = len(data)\n",
    "        label_counts = {}\n",
    "        for feature in data:\n",
    "            # 获得标签\n",
    "            one_label = feature[-1]\n",
    "            # 如果标签不在新定义的字典里则创建该标签\n",
    "            label_counts.setdefault(one_label, 0) \n",
    "            # 该类标签下含有数据的个数 \n",
    "            label_counts[one_label] += 1\n",
    "        # 计算数据集整体的信息熵\n",
    "        entropy = 0.0\n",
    "        for key in label_counts:\n",
    "            # 同类标签出现的概率\n",
    "            prob = float(label_counts[key]) / num_data\n",
    "            # 以2为底求对数\n",
    "            entropy -= prob * math.log(prob, 2)\n",
    "        return entropy\n",
    "\n",
    "    # 按照调和信息熵最小化原则分割数据集\n",
    "    def split(self, data):\n",
    "        # inf为正无穷大\n",
    "        min_entropy = np.inf\n",
    "        # 记录最终分割索引,记录切割点\n",
    "        index = -1\n",
    "        # 按照第一列对数据进行排序\n",
    "        sort_data = data[np.argsort(data[:, 0])]\n",
    "        # 初始化最终分割数据后的熵\n",
    "        last_e1, last_e2 = -1, -1\n",
    "        # 返回的数据结构，包含数据和对应的熵\n",
    "        S1 = dict()\n",
    "        S2 = dict()\n",
    "        for i in range(len(sort_data)):\n",
    "            # 分割数据集\n",
    "            split_data_1, split_data_2 = sort_data[: i + 1], sort_data[i + 1 :] \n",
    "            entropy1, entropy2 = ( self.cal_entropy(split_data_1), self.cal_entropy(split_data_2) ) # 计算信息熵\n",
    "            entropy = entropy1 * len(split_data_1) / len(sort_data) +  entropy2 * len( split_data_2) / len(sort_data)\n",
    "            # 如果加权熵小于最小值\n",
    "            if entropy < min_entropy:\n",
    "                min_entropy = entropy \n",
    "                index = i\n",
    "                last_e1 = entropy1 \n",
    "                last_e2 = entropy2\n",
    "        S1[\"entropy\"] = last_e1\n",
    "        S1[\"data\"] = sort_data[: index + 1]\n",
    "        S2[\"entropy\"] = last_e2\n",
    "        S2[\"data\"] = sort_data[index + 1 :]\n",
    "        return S1, S2, entropy\n",
    "    \n",
    "    # 对数据进行分组\n",
    "    def train(self, data):\n",
    "        # 需要遍历的 key\n",
    "        need_split_key = [0]\n",
    "        # 将整个数据作为一组\n",
    "        self.result.setdefault(0, {}) \n",
    "        self.result[0][\"entropy\"] = np.inf \n",
    "        self.result[0][\"data\"] = data\n",
    "        group = 1\n",
    "        for key in need_split_key:\n",
    "            S1, S2, entropy = self.split(self.result[key][\"data\"])\n",
    "            # 如果满足条件\n",
    "            if entropy > self.min_threshold or group < self.max_group:\n",
    "                self.result[key] = S1\n",
    "                print(key)\n",
    "                newKey = max(self.result.keys()) + 1 \n",
    "                print(newKey)\n",
    "                self.result[newKey] = S2\n",
    "                print(self.result[newKey])\n",
    "                need_split_key.extend([key]) \n",
    "                need_split_key.extend([newKey])\n",
    "                print(need_split_key)\n",
    "                group += 1\n",
    "            else: \n",
    "                break                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "{'entropy': 0.6840384356390417, 'data': array([[ 342,    1],\n",
      "       [ 361,    1],\n",
      "       [ 453,    1],\n",
      "       [ 561,    1],\n",
      "       [ 592,    1],\n",
      "       [ 641,    1],\n",
      "       [ 764,    0],\n",
      "       [1210,    1],\n",
      "       [2323,    0],\n",
      "       [2398,    1],\n",
      "       [2764,    1]])}\n",
      "[0, 0, 1]\n",
      "0\n",
      "2\n",
      "{'entropy': 0.0, 'data': array([[129,   0],\n",
      "       [164,   0],\n",
      "       [222,   0]])}\n",
      "[0, 0, 1, 0, 2]\n",
      "1\n",
      "3\n",
      "{'entropy': 0.9709505944546686, 'data': array([[ 764,    0],\n",
      "       [1210,    1],\n",
      "       [2323,    0],\n",
      "       [2398,    1],\n",
      "       [2764,    1]])}\n",
      "[0, 0, 1, 0, 2, 1, 3]\n",
      "0\n",
      "4\n",
      "{'entropy': 0.863120568566631, 'data': array([[ 49,   1],\n",
      "       [ 56,   1],\n",
      "       [ 63,   0],\n",
      "       [ 87,   1],\n",
      "       [ 88,   1],\n",
      "       [ 97,   0],\n",
      "       [121,   1]])}\n",
      "[0, 0, 1, 0, 2, 1, 3, 0, 4]\n",
      "2\n",
      "5\n",
      "{'entropy': 0.0, 'data': array([[164,   0],\n",
      "       [222,   0]])}\n",
      "[0, 0, 1, 0, 2, 1, 3, 0, 4, 2, 5]\n",
      "result is {0: {'entropy': 0.8112781244591328, 'data': array([[ 9,  0],\n",
      "       [10,  1],\n",
      "       [23,  0],\n",
      "       [28,  0]])}, 1: {'entropy': 0.0, 'data': array([[342,   1],\n",
      "       [361,   1],\n",
      "       [453,   1],\n",
      "       [561,   1],\n",
      "       [592,   1],\n",
      "       [641,   1]])}, 2: {'entropy': 0.0, 'data': array([[129,   0]])}, 3: {'entropy': 0.9709505944546686, 'data': array([[ 764,    0],\n",
      "       [1210,    1],\n",
      "       [2323,    0],\n",
      "       [2398,    1],\n",
      "       [2764,    1]])}, 4: {'entropy': 0.863120568566631, 'data': array([[ 49,   1],\n",
      "       [ 56,   1],\n",
      "       [ 63,   0],\n",
      "       [ 87,   1],\n",
      "       [ 88,   1],\n",
      "       [ 97,   0],\n",
      "       [121,   1]])}, 5: {'entropy': 0.0, 'data': array([[164,   0],\n",
      "       [222,   0]])}}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dse = DataSplitByEntropy(group=6,threshold=0.3) \n",
    "    data = dse.load_data()\n",
    "    dse.train(data)\n",
    "    print(\"result is {}\".format(dse.result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data =np.array(\n",
    "            [\n",
    "                [56, 1],   [87, 1],  [129, 0],  [23, 0],   [342, 1], \n",
    "                [641, 1],  [63, 0],  [2764, 1], [2323, 0], [453, 1], \n",
    "                [10, 1],   [9, 0],   [88, 1],   [222, 0],  [97, 0],\n",
    "                [2398, 1], [592, 1], [561, 1],  [764, 0],  [121, 1],\n",
    "                [28, 0],   [49, 1],  [361, 1],  [164, 0],  [1210, 1]\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  56,    1],\n",
       "       [  87,    1],\n",
       "       [ 129,    0],\n",
       "       [  23,    0],\n",
       "       [ 342,    1],\n",
       "       [ 641,    1],\n",
       "       [  63,    0],\n",
       "       [2764,    1],\n",
       "       [2323,    0],\n",
       "       [ 453,    1],\n",
       "       [  10,    1],\n",
       "       [   9,    0],\n",
       "       [  88,    1],\n",
       "       [ 222,    0],\n",
       "       [  97,    0],\n",
       "       [2398,    1],\n",
       "       [ 592,    1],\n",
       "       [ 561,    1],\n",
       "       [ 764,    0],\n",
       "       [ 121,    1],\n",
       "       [  28,    0],\n",
       "       [  49,    1],\n",
       "       [ 361,    1],\n",
       "       [ 164,    0],\n",
       "       [1210,    1]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   9,    0],\n",
       "       [  10,    1],\n",
       "       [  23,    0],\n",
       "       [  28,    0],\n",
       "       [  49,    1],\n",
       "       [  56,    1],\n",
       "       [  63,    0],\n",
       "       [  87,    1],\n",
       "       [  88,    1],\n",
       "       [  97,    0],\n",
       "       [ 121,    1],\n",
       "       [ 129,    0],\n",
       "       [ 164,    0],\n",
       "       [ 222,    0],\n",
       "       [ 342,    1],\n",
       "       [ 361,    1],\n",
       "       [ 453,    1],\n",
       "       [ 561,    1],\n",
       "       [ 592,    1],\n",
       "       [ 641,    1],\n",
       "       [ 764,    0],\n",
       "       [1210,    1],\n",
       "       [2323,    0],\n",
       "       [2398,    1],\n",
       "       [2764,    1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[np.argsort(data[:, 0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts={}\n",
    "for feature in data:\n",
    "            # 获得标签\n",
    "    one_label = feature[-1]\n",
    "            # 如果标签不在新定义的字典里则创建该标签\n",
    "    label_counts.setdefault(one_label, 0) \n",
    "            # 该类标签下含有数据的个数 \n",
    "    label_counts[one_label] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 15, 0: 10}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = {}\n",
    "result.setdefault(0,{})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'entropy': inf,\n",
       "  'data': array([[  56,    1],\n",
       "         [  87,    1],\n",
       "         [ 129,    0],\n",
       "         [  23,    0],\n",
       "         [ 342,    1],\n",
       "         [ 641,    1],\n",
       "         [  63,    0],\n",
       "         [2764,    1],\n",
       "         [2323,    0],\n",
       "         [ 453,    1],\n",
       "         [  10,    1],\n",
       "         [   9,    0],\n",
       "         [  88,    1],\n",
       "         [ 222,    0],\n",
       "         [  97,    0],\n",
       "         [2398,    1],\n",
       "         [ 592,    1],\n",
       "         [ 561,    1],\n",
       "         [ 764,    0],\n",
       "         [ 121,    1],\n",
       "         [  28,    0],\n",
       "         [  49,    1],\n",
       "         [ 361,    1],\n",
       "         [ 164,    0],\n",
       "         [1210,    1]])}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0][\"entropy\"] = np.inf\n",
    "result[0][\"data\"] = data\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_entropy(data):\n",
    "    num_data = len(data)\n",
    "    label_counts = {}\n",
    "    for feature in data:\n",
    "        # 获得标签\n",
    "        one_label = feature[-1]\n",
    "        # 如果标签不在新定义的字典里则创建该标签\n",
    "        label_counts.setdefault(one_label, 0) \n",
    "        # 该类标签下含有数据的个数 \n",
    "        label_counts[one_label] += 1\n",
    "    # 计算数据集整体的信息熵\n",
    "    entropy = 0.0\n",
    "    for key in label_counts:\n",
    "        # 同类标签出现的概率\n",
    "        prob = float(label_counts[key]) / num_data\n",
    "        # 以2为底求对数\n",
    "        entropy -= prob * math.log(prob, 2)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(data):\n",
    "    # inf为正无穷大\n",
    "    min_entropy = np.inf\n",
    "    # 记录最终分割索引\n",
    "    index = -1\n",
    "    # 按照第一列对数据进行排序\n",
    "    sort_data = data[np.argsort(data[:, 0])]\n",
    "    # 初始化最终分割数据后的熵\n",
    "    last_e1, last_e2 = -1, -1\n",
    "    # 返回的数据结构，包含数据和对应的熵\n",
    "    S1 = dict()\n",
    "    S2 = dict()\n",
    "    for i in range(len(sort_data)):\n",
    "        # 分割数据集\n",
    "        split_data_1, split_data_2 = sort_data[: i + 1], sort_data[i + 1 :] \n",
    "        entropy1, entropy2 = (cal_entropy(split_data_1), cal_entropy(split_data_2) ) # 计算信息熵\n",
    "        entropy = entropy1 * len(split_data_1) / len(sort_data) +  entropy2 * len( split_data_2) / len(sort_data)\n",
    "        # 如果加权平均熵小于最小值\n",
    "        if entropy < min_entropy:\n",
    "            min_entropy = entropy \n",
    "            index = i\n",
    "            last_e1 = entropy1 \n",
    "            last_e2 = entropy2\n",
    "    S1[\"entropy\"] = last_e1\n",
    "    S1[\"data\"] = sort_data[: index + 1]\n",
    "    S2[\"entropy\"] = last_e2\n",
    "    S2[\"data\"] = sort_data[index + 1 :]\n",
    "    return S1, S2, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "S1, S2, entropy = split(result[0][\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'entropy': 0.9852281360342516,\n",
       "  'data': array([[  9,   0],\n",
       "         [ 10,   1],\n",
       "         [ 23,   0],\n",
       "         [ 28,   0],\n",
       "         [ 49,   1],\n",
       "         [ 56,   1],\n",
       "         [ 63,   0],\n",
       "         [ 87,   1],\n",
       "         [ 88,   1],\n",
       "         [ 97,   0],\n",
       "         [121,   1],\n",
       "         [129,   0],\n",
       "         [164,   0],\n",
       "         [222,   0]])},\n",
       " {'entropy': 0.6840384356390417,\n",
       "  'data': array([[ 342,    1],\n",
       "         [ 361,    1],\n",
       "         [ 453,    1],\n",
       "         [ 561,    1],\n",
       "         [ 592,    1],\n",
       "         [ 641,    1],\n",
       "         [ 764,    0],\n",
       "         [1210,    1],\n",
       "         [2323,    0],\n",
       "         [2398,    1],\n",
       "         [2764,    1]])})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S1, S2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "need_split_key = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0] = S1\n",
    "newKey = max(result.keys()) + 1 \n",
    "result[newKey] = S2\n",
    "need_split_key.extend([0]) \n",
    "need_split_key.extend([newKey])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'entropy': 0.9852281360342516,\n",
       "  'data': array([[  9,   0],\n",
       "         [ 10,   1],\n",
       "         [ 23,   0],\n",
       "         [ 28,   0],\n",
       "         [ 49,   1],\n",
       "         [ 56,   1],\n",
       "         [ 63,   0],\n",
       "         [ 87,   1],\n",
       "         [ 88,   1],\n",
       "         [ 97,   0],\n",
       "         [121,   1],\n",
       "         [129,   0],\n",
       "         [164,   0],\n",
       "         [222,   0]])},\n",
       " 1: {'entropy': 0.6840384356390417,\n",
       "  'data': array([[ 342,    1],\n",
       "         [ 361,    1],\n",
       "         [ 453,    1],\n",
       "         [ 561,    1],\n",
       "         [ 592,    1],\n",
       "         [ 641,    1],\n",
       "         [ 764,    0],\n",
       "         [1210,    1],\n",
       "         [2323,    0],\n",
       "         [2398,    1],\n",
       "         [2764,    1]])}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "need_split_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

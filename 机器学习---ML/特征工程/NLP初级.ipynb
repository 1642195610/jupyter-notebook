{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV \\\n",
    ",train_test_split\n",
    "from sklearn.feature_extraction.text import \\\n",
    "CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99984</th>\n",
       "      <td>99996</td>\n",
       "      <td>0</td>\n",
       "      <td>@Cupcake  seems like a repeating problem   hop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99985</th>\n",
       "      <td>99997</td>\n",
       "      <td>1</td>\n",
       "      <td>@cupcake__ arrrr we both replied to each other...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99986</th>\n",
       "      <td>99998</td>\n",
       "      <td>0</td>\n",
       "      <td>@CuPcAkE_2120 ya i thought so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99987</th>\n",
       "      <td>99999</td>\n",
       "      <td>1</td>\n",
       "      <td>@Cupcake_Dollie Yes. Yes. I'm glad you had mor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99988</th>\n",
       "      <td>100000</td>\n",
       "      <td>1</td>\n",
       "      <td>@cupcake_kayla haha yes you do</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99989 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ItemID  Sentiment                                      SentimentText\n",
       "0           1          0                       is so sad for my APL frie...\n",
       "1           2          0                     I missed the New Moon trail...\n",
       "2           3          1                            omg its already 7:30 :O\n",
       "3           4          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4           5          0           i think mi bf is cheating on me!!!   ...\n",
       "...       ...        ...                                                ...\n",
       "99984   99996          0  @Cupcake  seems like a repeating problem   hop...\n",
       "99985   99997          1  @cupcake__ arrrr we both replied to each other...\n",
       "99986   99998          0                     @CuPcAkE_2120 ya i thought so \n",
       "99987   99999          1  @Cupcake_Dollie Yes. Yes. I'm glad you had mor...\n",
       "99988  100000          1                    @cupcake_kayla haha yes you do \n",
       "\n",
       "[99989 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = pd.read_csv(\"情感文本.csv\",encoding = 'latin1')\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99989, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                     is so sad for my APL friend.............'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s['SentimentText'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99984</th>\n",
       "      <td>0</td>\n",
       "      <td>@Cupcake  seems like a repeating problem   hop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99985</th>\n",
       "      <td>1</td>\n",
       "      <td>@cupcake__ arrrr we both replied to each other...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99986</th>\n",
       "      <td>0</td>\n",
       "      <td>@CuPcAkE_2120 ya i thought so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99987</th>\n",
       "      <td>1</td>\n",
       "      <td>@Cupcake_Dollie Yes. Yes. I'm glad you had mor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99988</th>\n",
       "      <td>1</td>\n",
       "      <td>@cupcake_kayla haha yes you do</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99989 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentiment                                      SentimentText\n",
       "0              0                       is so sad for my APL frie...\n",
       "1              0                     I missed the New Moon trail...\n",
       "2              1                            omg its already 7:30 :O\n",
       "3              0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4              0           i think mi bf is cheating on me!!!   ...\n",
       "...          ...                                                ...\n",
       "99984          0  @Cupcake  seems like a repeating problem   hop...\n",
       "99985          1  @cupcake__ arrrr we both replied to each other...\n",
       "99986          0                     @CuPcAkE_2120 ya i thought so \n",
       "99987          1  @Cupcake_Dollie Yes. Yes. I'm glad you had mor...\n",
       "99988          1                    @cupcake_kayla haha yes you do \n",
       "\n",
       "[99989 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd = s.drop(labels = ['ItemID'],axis = 1)\n",
    "sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import \\\n",
    "CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\"\n",
    "]\n",
    "vectorizer = CountVectorizer()\n",
    "x = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "# ['and','document','first','is','one','second', \\\n",
    "#  'the','third','this']\n",
    "print(x.toarray())\n",
    "# 每个单词在第几句话出现几次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 1)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 1)\t2\n",
      "  (1, 5)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 4)\t1\n",
      "  (3, 8)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 6)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 1)\t1\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(x.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = sd['SentimentText']\n",
    "y = sd['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '0001t',\n",
       " '000martha',\n",
       " '001',\n",
       " '0010x0010',\n",
       " '0022',\n",
       " '007',\n",
       " '007heather007',\n",
       " '007hil',\n",
       " '007jbond',\n",
       " '007louiseob',\n",
       " '007peter',\n",
       " '00a',\n",
       " '00am',\n",
       " '00fieldsy',\n",
       " '00geneva',\n",
       " '00h',\n",
       " '00k',\n",
       " '00mathias',\n",
       " '00mony00',\n",
       " '00neji',\n",
       " '00pet00',\n",
       " '00pm',\n",
       " '00ps',\n",
       " '00seve',\n",
       " '00vicky00',\n",
       " '01',\n",
       " '01000101',\n",
       " '011',\n",
       " '011iver',\n",
       " '0130',\n",
       " '0132',\n",
       " '016578',\n",
       " '017',\n",
       " '01jamcon',\n",
       " '01movie',\n",
       " '01ps',\n",
       " '01theone',\n",
       " '02',\n",
       " '0205laura',\n",
       " '021',\n",
       " '0239',\n",
       " '0292070616',\n",
       " '02plane',\n",
       " '02sweetpea02',\n",
       " '03',\n",
       " '0309love',\n",
       " '03l',\n",
       " '03robin',\n",
       " '04',\n",
       " '0411247113',\n",
       " '0433963407',\n",
       " '044',\n",
       " '0448417513',\n",
       " '04pm',\n",
       " '04youenc',\n",
       " '05',\n",
       " '0546',\n",
       " '05jstone',\n",
       " '05pm',\n",
       " '06',\n",
       " '061004',\n",
       " '0631',\n",
       " '06atmfj',\n",
       " '06eleven',\n",
       " '06falloutgirl06',\n",
       " '07',\n",
       " '0700',\n",
       " '071',\n",
       " '07am',\n",
       " '07nick07',\n",
       " '07thking',\n",
       " '07yez',\n",
       " '08',\n",
       " '0811thereason',\n",
       " '083lxj',\n",
       " '087r9',\n",
       " '08erica09',\n",
       " '08fjw',\n",
       " '08h8a',\n",
       " '08home',\n",
       " '08sikid',\n",
       " '08zlk',\n",
       " '09',\n",
       " '09011',\n",
       " '090595',\n",
       " '093m3',\n",
       " '094459',\n",
       " '09aoc',\n",
       " '09casper',\n",
       " '09ch',\n",
       " '09mileycyrusfan',\n",
       " '0_o',\n",
       " '0_oi',\n",
       " '0again',\n",
       " '0amityville0',\n",
       " '0boy',\n",
       " '0bscenity',\n",
       " '0bxmudm0mma',\n",
       " '0c6abp',\n",
       " '0cl0ver',\n",
       " '0colleen',\n",
       " '0dds0cksmyelb0w',\n",
       " '0dpg9z',\n",
       " '0e7bethany',\n",
       " '0f',\n",
       " '0h',\n",
       " '0halysssa',\n",
       " '0helenka',\n",
       " '0herica',\n",
       " '0hsheila',\n",
       " '0hshit_itskirst',\n",
       " '0hyoshimi',\n",
       " '0j90oj',\n",
       " '0jjqkej',\n",
       " '0kat0',\n",
       " '0kjygqj',\n",
       " '0kn0',\n",
       " '0ktimeforplanb',\n",
       " '0ladyb0',\n",
       " '0lli',\n",
       " '0mg1tsm3g4n',\n",
       " '0mgiitzree',\n",
       " '0mie',\n",
       " '0mnislasher',\n",
       " '0music_freak0',\n",
       " '0ne',\n",
       " '0nlihuman',\n",
       " '0nlyindreams',\n",
       " '0o0o0oh',\n",
       " '0omiriamo0',\n",
       " '0otlo0',\n",
       " '0oze',\n",
       " '0pensource',\n",
       " '0princess0',\n",
       " '0ptimo',\n",
       " '0px',\n",
       " '0qu92j',\n",
       " '0r',\n",
       " '0rch1d',\n",
       " '0sn',\n",
       " '0ssi',\n",
       " '0summerbreeze0',\n",
       " '0taqueen',\n",
       " '0ut0f_thisw0rld',\n",
       " '0v3rdz',\n",
       " '0x6d6172696f',\n",
       " '0xcafebabe',\n",
       " '0xdeadbeef',\n",
       " '0y',\n",
       " '0zark',\n",
       " '0zmvnj',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '10000',\n",
       " '1000000',\n",
       " '10000000x',\n",
       " '1000014',\n",
       " '1000__monkeys',\n",
       " '1000cigarettes',\n",
       " '1000markets',\n",
       " '1000mg',\n",
       " '1000mgofjenn',\n",
       " '1000ms',\n",
       " '1000s',\n",
       " '1000th',\n",
       " '1000wattmarc',\n",
       " '1000xs',\n",
       " '1001',\n",
       " '1001noisycamera',\n",
       " '100432',\n",
       " '100acre',\n",
       " '100bpm',\n",
       " '100daysoff',\n",
       " '100ftzombie',\n",
       " '100gb',\n",
       " '100interviews',\n",
       " '100k',\n",
       " '100kanisha100',\n",
       " '100konacoffee',\n",
       " '100laura',\n",
       " '100legs',\n",
       " '100m',\n",
       " '100miles',\n",
       " '100miletraining',\n",
       " '100monkeysmusic',\n",
       " '100pgs',\n",
       " '100pushups',\n",
       " '100th',\n",
       " '100times',\n",
       " '100x',\n",
       " '101',\n",
       " '1014511',\n",
       " '1015jamz',\n",
       " '101887',\n",
       " '1019mixchicago',\n",
       " '101denise',\n",
       " '101nine',\n",
       " '101ofawolf',\n",
       " '101wrif',\n",
       " '102',\n",
       " '1021studios',\n",
       " '1027kiisfm',\n",
       " '102megan',\n",
       " '103',\n",
       " '1030',\n",
       " '10379960',\n",
       " '1039thejuice',\n",
       " '1040ez',\n",
       " '1041pm',\n",
       " '1043dennis',\n",
       " '1045',\n",
       " '1045chumfm',\n",
       " '10478',\n",
       " '1047fishtraffic',\n",
       " '1049thewolf',\n",
       " '104bob',\n",
       " '105',\n",
       " '1059starfm',\n",
       " '106',\n",
       " '1067kissfm',\n",
       " '106andpark',\n",
       " '106egw',\n",
       " '106jackfm',\n",
       " '106park_videos',\n",
       " '107',\n",
       " '1075kzl',\n",
       " '1079the',\n",
       " '108',\n",
       " '1080i',\n",
       " '1080p',\n",
       " '1082118',\n",
       " '108radha',\n",
       " '109',\n",
       " '1090527',\n",
       " '10a',\n",
       " '10am',\n",
       " '10and5',\n",
       " '10bagspacking',\n",
       " '10celcius',\n",
       " '10days',\n",
       " '10enigma',\n",
       " '10g',\n",
       " '10gpwq',\n",
       " '10h30m',\n",
       " '10h4ne',\n",
       " '10ish',\n",
       " '10isjess',\n",
       " '10k',\n",
       " '10km',\n",
       " '10kserversmilestone',\n",
       " '10lb',\n",
       " '10lbs',\n",
       " '10m',\n",
       " '10mg',\n",
       " '10min',\n",
       " '10mins',\n",
       " '10naxr',\n",
       " '10news',\n",
       " '10o8lw',\n",
       " '10p',\n",
       " '10pm',\n",
       " '10rdben',\n",
       " '10smjp',\n",
       " '10th',\n",
       " '10thedoctor10',\n",
       " '10thmuse',\n",
       " '10thtothefraser',\n",
       " '10ufry',\n",
       " '10veisonitsway',\n",
       " '10x',\n",
       " '10yds',\n",
       " '10yearsofenema',\n",
       " '10yoface',\n",
       " '10yrs',\n",
       " '10zlaine',\n",
       " '10ï',\n",
       " '11',\n",
       " '110',\n",
       " '11028',\n",
       " '11072cm',\n",
       " '1111',\n",
       " '1111am',\n",
       " '1111th',\n",
       " '111adam',\n",
       " '112',\n",
       " '1129kydd',\n",
       " '112mirabela',\n",
       " '113',\n",
       " '1130',\n",
       " '11327',\n",
       " '1137',\n",
       " '11394607',\n",
       " '113oy3',\n",
       " '113wgb',\n",
       " '114',\n",
       " '115',\n",
       " '115381465',\n",
       " '116',\n",
       " '11640447',\n",
       " '117',\n",
       " '117cecilia',\n",
       " '117e',\n",
       " '117ner',\n",
       " '117qxq',\n",
       " '118',\n",
       " '118247',\n",
       " '11866t',\n",
       " '119',\n",
       " '1192tom',\n",
       " '1193st',\n",
       " '1199',\n",
       " '11_26',\n",
       " '11a',\n",
       " '11am',\n",
       " '11aslk',\n",
       " '11bc8p',\n",
       " '11bdia',\n",
       " '11bpr9',\n",
       " '11bqzw',\n",
       " '11chiqa11',\n",
       " '11devon',\n",
       " '11dmce',\n",
       " '11ehft',\n",
       " '11forgottenlaws',\n",
       " '11hunabku11',\n",
       " '11ish',\n",
       " '11locs',\n",
       " '11mm',\n",
       " '11oa8',\n",
       " '11oivn',\n",
       " '11p3l9',\n",
       " '11phoenix',\n",
       " '11pm',\n",
       " '11points',\n",
       " '11sadia',\n",
       " '11th',\n",
       " '11th_echo',\n",
       " '11thward',\n",
       " '11twenty',\n",
       " '11wdnick',\n",
       " '11yebbea',\n",
       " '11zyzl',\n",
       " '12',\n",
       " '120',\n",
       " '1200',\n",
       " '12071',\n",
       " '12072',\n",
       " '12074',\n",
       " '12081',\n",
       " '12086',\n",
       " '1208wonderful',\n",
       " '120gb',\n",
       " '120k',\n",
       " '120mm',\n",
       " '120n2f',\n",
       " '120politics',\n",
       " '122',\n",
       " '1225491',\n",
       " '12275',\n",
       " '122990476',\n",
       " '123',\n",
       " '1230',\n",
       " '1230lz',\n",
       " '1234569or10',\n",
       " '123deanisme',\n",
       " '123itsmemary',\n",
       " '123lottie',\n",
       " '123pricecheck',\n",
       " '123print',\n",
       " '123zyke',\n",
       " '12437',\n",
       " '1245',\n",
       " '1248wjp',\n",
       " '1249ck',\n",
       " '125',\n",
       " '127roseavenue',\n",
       " '128',\n",
       " '12800',\n",
       " '1280x1024',\n",
       " '1280x800',\n",
       " '128pc',\n",
       " '129',\n",
       " '129lb_pup',\n",
       " '12am',\n",
       " '12baroriginal',\n",
       " '12c31e',\n",
       " '12c4',\n",
       " '12egv',\n",
       " '12eyes',\n",
       " '12h',\n",
       " '12hour',\n",
       " '12isthisthingon',\n",
       " '12k',\n",
       " '12kyle',\n",
       " '12magazine',\n",
       " '12maureen21',\n",
       " '12monkeys',\n",
       " '12mph',\n",
       " '12np1h',\n",
       " '12oguz',\n",
       " '12pm',\n",
       " '12serendipity21',\n",
       " '12stones',\n",
       " '12th',\n",
       " '12thmanrising',\n",
       " '12thplanet',\n",
       " '12uryh',\n",
       " '12wfz',\n",
       " '12yo',\n",
       " '12yrha',\n",
       " '12â',\n",
       " '13',\n",
       " '130',\n",
       " '1300',\n",
       " '13019',\n",
       " '130587',\n",
       " '13092293',\n",
       " '130am',\n",
       " '1313',\n",
       " '1320',\n",
       " '1325dq',\n",
       " '133326',\n",
       " '13341015518',\n",
       " '1337357',\n",
       " '1337_musician',\n",
       " '1337sauce',\n",
       " '1337th',\n",
       " '1339',\n",
       " '134',\n",
       " '1347',\n",
       " '1349',\n",
       " '135',\n",
       " '1350',\n",
       " '136',\n",
       " '137',\n",
       " '137436',\n",
       " '13850',\n",
       " '1395',\n",
       " '13b96d',\n",
       " '13bdesign',\n",
       " '13be0',\n",
       " '13beatking',\n",
       " '13bgd9',\n",
       " '13c',\n",
       " '13c7l9',\n",
       " '13christina',\n",
       " '13curses',\n",
       " '13dqv5',\n",
       " '13ecw',\n",
       " '13g',\n",
       " '13hours',\n",
       " '13i5z',\n",
       " '13jessrocks13',\n",
       " '13jessrocks31',\n",
       " '13k',\n",
       " '13kz7',\n",
       " '13lbs',\n",
       " '13loka',\n",
       " '13mario',\n",
       " '13monsters',\n",
       " '13music',\n",
       " '13musiconline',\n",
       " '13n7iy',\n",
       " '13nard',\n",
       " '13nikki',\n",
       " '13ntp',\n",
       " '13oiyu',\n",
       " '13pcwj',\n",
       " '13riandavis',\n",
       " '13riots',\n",
       " '13s5mg',\n",
       " '13th',\n",
       " '13thfloornyc',\n",
       " '13thoughts',\n",
       " '13twelve',\n",
       " '13uckshot',\n",
       " '13yrold',\n",
       " '14',\n",
       " '140',\n",
       " '1400',\n",
       " '14000',\n",
       " '140208',\n",
       " '140798',\n",
       " '1407graymalkin',\n",
       " '140c',\n",
       " '140conf',\n",
       " '140horror',\n",
       " '140lovebird',\n",
       " '140lover',\n",
       " '140smiles',\n",
       " '140tc',\n",
       " '14191',\n",
       " '14226',\n",
       " '14229',\n",
       " '14230',\n",
       " '1424',\n",
       " '14245',\n",
       " '14248',\n",
       " '14260',\n",
       " '14279',\n",
       " '14284',\n",
       " '14299',\n",
       " '142staircases',\n",
       " '14300',\n",
       " '143932',\n",
       " '143cherelle',\n",
       " '143presents',\n",
       " '144',\n",
       " '1443',\n",
       " '145',\n",
       " '146',\n",
       " '1468589659',\n",
       " '146ac',\n",
       " '148325',\n",
       " '1487pl',\n",
       " '148apps',\n",
       " '149',\n",
       " '1496429',\n",
       " '14c4n7',\n",
       " '14eleven',\n",
       " '14h',\n",
       " '14hrs',\n",
       " '14iztb',\n",
       " '14kt',\n",
       " '14mph',\n",
       " '14noi0',\n",
       " '14ox5g',\n",
       " '14pepsicans',\n",
       " '14pm',\n",
       " '14qas',\n",
       " '14qctk',\n",
       " '14rvft',\n",
       " '14s',\n",
       " '14th',\n",
       " '14thish',\n",
       " '14throad',\n",
       " '14tonystewart',\n",
       " '14txjz',\n",
       " '15',\n",
       " '150',\n",
       " '1500',\n",
       " '15000',\n",
       " '1500k',\n",
       " '1500ornothin',\n",
       " '150mb',\n",
       " '1525',\n",
       " '154014',\n",
       " '154056',\n",
       " '15425876',\n",
       " '1544jh',\n",
       " '155',\n",
       " '1554',\n",
       " '15627',\n",
       " '15659',\n",
       " '1566',\n",
       " '15660',\n",
       " '157',\n",
       " '157w3i',\n",
       " '158',\n",
       " '1580px',\n",
       " '1586789',\n",
       " '159472',\n",
       " '15am',\n",
       " '15c',\n",
       " '15cg',\n",
       " '15cm',\n",
       " '15g0h4',\n",
       " '15grqc',\n",
       " '15ha0j',\n",
       " '15hrs',\n",
       " '15ina7',\n",
       " '15june',\n",
       " '15jyce',\n",
       " '15k',\n",
       " '15m',\n",
       " '15mab',\n",
       " '15mins',\n",
       " '15minsofmetal',\n",
       " '15p',\n",
       " '15pfx',\n",
       " '15pm',\n",
       " '15rn',\n",
       " '15ssci',\n",
       " '15syu',\n",
       " '15th',\n",
       " '15tp6e',\n",
       " '15tpjc',\n",
       " '15vs',\n",
       " '15x3yw',\n",
       " '15yrs',\n",
       " '16',\n",
       " '160',\n",
       " '1600',\n",
       " '1600milesaway',\n",
       " '1600msp',\n",
       " '160210',\n",
       " '16035',\n",
       " '160509',\n",
       " '160641',\n",
       " '1608',\n",
       " '16089',\n",
       " '160916',\n",
       " '160gb',\n",
       " '1611',\n",
       " '16160',\n",
       " '16365',\n",
       " '164',\n",
       " '165',\n",
       " '167',\n",
       " '1686296573',\n",
       " '168e97m',\n",
       " '16_mileycyrus',\n",
       " '16am',\n",
       " '16barz',\n",
       " '16bvx7',\n",
       " '16eqgo',\n",
       " '16frames',\n",
       " '16g',\n",
       " '16gb',\n",
       " '16k',\n",
       " '16l1a3',\n",
       " '16mb',\n",
       " '16mbps',\n",
       " '16missedcalls',\n",
       " '16ogje',\n",
       " '16pebbles',\n",
       " '16pm',\n",
       " '16stargirl16',\n",
       " '16stars',\n",
       " '16th',\n",
       " '16thstreetj',\n",
       " '16vkhs',\n",
       " '16wks',\n",
       " '16zzj7',\n",
       " '17',\n",
       " '170',\n",
       " '1700',\n",
       " '1701',\n",
       " '1708',\n",
       " '170cm',\n",
       " '1737',\n",
       " '175',\n",
       " '1750',\n",
       " '177',\n",
       " '177upw',\n",
       " '1785ug',\n",
       " '178e3c',\n",
       " '178th',\n",
       " '179th',\n",
       " '17am',\n",
       " '17clwm',\n",
       " '17dgxc',\n",
       " '17eo4',\n",
       " '17esgw',\n",
       " '17f19k',\n",
       " '17gr8',\n",
       " '17hmr',\n",
       " '17ifji',\n",
       " '17k',\n",
       " '17m',\n",
       " '17p',\n",
       " '17rs',\n",
       " '17sgd',\n",
       " '17t1fa',\n",
       " '17th',\n",
       " '17thsnoop',\n",
       " '17uv85',\n",
       " '17wjag',\n",
       " '18',\n",
       " '180',\n",
       " '1800',\n",
       " '1800flowers',\n",
       " '1800katiecat',\n",
       " '1800ll',\n",
       " '1808',\n",
       " '180ylw',\n",
       " '180ë',\n",
       " '181',\n",
       " '18145',\n",
       " '182',\n",
       " '1825remnant',\n",
       " '182dany',\n",
       " '18312',\n",
       " '184948',\n",
       " '1859733',\n",
       " '1864ml',\n",
       " '187',\n",
       " '1898553',\n",
       " '1899',\n",
       " '189rhl',\n",
       " '18am',\n",
       " '18bikes',\n",
       " '18c',\n",
       " '18g6ku',\n",
       " '18h',\n",
       " '18h30m',\n",
       " '18hrs',\n",
       " '18ox6k',\n",
       " '18percentgrey',\n",
       " '18s',\n",
       " '18sjxi',\n",
       " '18th',\n",
       " '18wheeleredy',\n",
       " '18xobk',\n",
       " '18yo',\n",
       " '18yr',\n",
       " '19',\n",
       " '190',\n",
       " '1900txts',\n",
       " '1903',\n",
       " '1908applesalmon',\n",
       " '1908prima',\n",
       " '1912',\n",
       " '1914',\n",
       " '1918',\n",
       " '192',\n",
       " '1920x1080',\n",
       " '1928',\n",
       " '1935',\n",
       " '1938',\n",
       " '1938media',\n",
       " '1945',\n",
       " '1947',\n",
       " '1960s',\n",
       " '19627',\n",
       " '1965',\n",
       " '1967',\n",
       " '1968',\n",
       " '196841',\n",
       " '1969',\n",
       " '1969jojo',\n",
       " '1970',\n",
       " '1970s',\n",
       " '1972',\n",
       " '1975',\n",
       " '1976',\n",
       " '1977',\n",
       " '1979',\n",
       " '198',\n",
       " '1980',\n",
       " '1980s',\n",
       " '1981',\n",
       " '1982',\n",
       " '1983',\n",
       " '1984',\n",
       " '1985',\n",
       " '1986',\n",
       " '1988',\n",
       " '1988lips',\n",
       " '1989',\n",
       " '199',\n",
       " '1994',\n",
       " '1995nat',\n",
       " '1995shand',\n",
       " '1996',\n",
       " '1997',\n",
       " '1998',\n",
       " '1999',\n",
       " '19_spotter_bg',\n",
       " '19alice95',\n",
       " '19antoinette84',\n",
       " '19blu4',\n",
       " '19c816tf9227',\n",
       " '19christopher92',\n",
       " '19dandan84',\n",
       " '19daveed84',\n",
       " '19fischi75',\n",
       " '19ibzn',\n",
       " '19mark90',\n",
       " '19markattack',\n",
       " '19orxq',\n",
       " '19oz',\n",
       " '19pm',\n",
       " '19poise_n_ivy08',\n",
       " '19s1lc',\n",
       " '19sensational08',\n",
       " '19shonda86',\n",
       " '19sites',\n",
       " '19sumblim87',\n",
       " '19th',\n",
       " '1_0chick',\n",
       " '1_0man',\n",
       " '1_2_manytweets',\n",
       " '1_50_1',\n",
       " '1_cali_baby',\n",
       " '1_dollar',\n",
       " '1_i_will_e',\n",
       " '1_jenny_love',\n",
       " '1_nac_08',\n",
       " '1_pink_fan',\n",
       " '1a02sp',\n",
       " '1a1une',\n",
       " '1a2a3a4a',\n",
       " '1a3vju',\n",
       " '1achilles1',\n",
       " '1adrianneal',\n",
       " '1afcz2',\n",
       " '1aicha',\n",
       " '1alscj',\n",
       " '1am',\n",
       " '1andonlydavid',\n",
       " '1andonlylavinia',\n",
       " '1angelinwaiting',\n",
       " '1anticancer',\n",
       " '1antidote',\n",
       " '1antsjp',\n",
       " '1anuw4',\n",
       " '1applegatec',\n",
       " '1ark2009',\n",
       " '1arner',\n",
       " '1articleperday',\n",
       " '1asmarie',\n",
       " '1azylizzie',\n",
       " '1b',\n",
       " '1badasschick',\n",
       " '1baiav',\n",
       " '1bambi',\n",
       " '1beachsax11',\n",
       " '1bep',\n",
       " '1bigcityboy',\n",
       " '1bigcurt',\n",
       " '1bjfib',\n",
       " '1bking',\n",
       " '1blessed4life',\n",
       " '1blkangel',\n",
       " '1bossychick',\n",
       " '1bucksone',\n",
       " '1business',\n",
       " '1bzhiker',\n",
       " '1c',\n",
       " '1cakebaker',\n",
       " '1caliboy',\n",
       " '1calorie',\n",
       " '1capplegate',\n",
       " '1cell',\n",
       " '1charlottemarie',\n",
       " '1chazd',\n",
       " '1chefette',\n",
       " '1chicmommy',\n",
       " '1cincymom',\n",
       " '1classediva',\n",
       " '1cloudstrife',\n",
       " '1cm',\n",
       " '1cor16_13',\n",
       " '1cr0d',\n",
       " '1crazyblonde',\n",
       " '1critic',\n",
       " '1cutechicwitfm',\n",
       " '1cutefakeblonde',\n",
       " '1d3cw',\n",
       " '1dashtube',\n",
       " '1davidn',\n",
       " '1day',\n",
       " '1dee2',\n",
       " '1destiny1life',\n",
       " '1doesnt',\n",
       " '1dozymoo',\n",
       " '1e0na',\n",
       " '1eighton',\n",
       " '1ele',\n",
       " '1eobe',\n",
       " '1eqag',\n",
       " '1f9lk7',\n",
       " '1familyfood',\n",
       " '1flyazzmami',\n",
       " '1flyharmony',\n",
       " '1fmjamz',\n",
       " '1freakofnature',\n",
       " '1gb',\n",
       " '1gemineye',\n",
       " '1ghz',\n",
       " '1girl2nv',\n",
       " '1gj',\n",
       " '1gregie',\n",
       " '1gtzcx',\n",
       " '1guvnor',\n",
       " '1h',\n",
       " '1heron',\n",
       " '1hipsterdoofus',\n",
       " '1hool4',\n",
       " '1hour',\n",
       " '1hr',\n",
       " '1hundredpercent',\n",
       " '1hunid',\n",
       " '1inamilliondime',\n",
       " '1inch',\n",
       " '1indienation',\n",
       " '1iranian',\n",
       " '1j9k9h0',\n",
       " '1jaredpadalecki',\n",
       " '1jessicasexybbe',\n",
       " '1jh',\n",
       " '1just',\n",
       " '1k',\n",
       " '1kali3',\n",
       " '1kalikatt1',\n",
       " '1kedup',\n",
       " '1kid1',\n",
       " '1kingjames',\n",
       " '1krazykorean',\n",
       " '1kris007',\n",
       " '1kushking',\n",
       " '1kuupua',\n",
       " '1ladii_ladiiway',\n",
       " '1lb',\n",
       " '1lenore',\n",
       " '1lindsaysmith',\n",
       " '1littlefish',\n",
       " '1livestew',\n",
       " '1lovedee',\n",
       " '1lovelaura',\n",
       " '1lovelydreamer',\n",
       " '1lutherblissett',\n",
       " '1luv633',\n",
       " '1lzqyl',\n",
       " '1m',\n",
       " '1m_a_dork',\n",
       " '1marc',\n",
       " '1marche',\n",
       " '1martymcfly',\n",
       " '1mgoldstars',\n",
       " '1miggs',\n",
       " '1mikeala1',\n",
       " '1milekyle',\n",
       " '1miletogo',\n",
       " '1mileyraycyrus1',\n",
       " '1min',\n",
       " '1mind1energy',\n",
       " '1mjritter',\n",
       " '1month',\n",
       " '1mpfa07',\n",
       " '1mrsrobinson',\n",
       " '1msdee',\n",
       " '1mwilson',\n",
       " '1mysterygirl',\n",
       " '1nationchris',\n",
       " '1naturesvariety',\n",
       " '1ndefinite',\n",
       " '1ndus',\n",
       " '1nonlymercy',\n",
       " '1norlm1',\n",
       " '1npd',\n",
       " '1ntv',\n",
       " '1nu',\n",
       " '1nvictus',\n",
       " '1o28n',\n",
       " '1objectivist',\n",
       " '1ofh',\n",
       " '1oju',\n",
       " '1omarion',\n",
       " '1op',\n",
       " '1ord',\n",
       " '1outside',\n",
       " '1ovakynd',\n",
       " '1ovesays',\n",
       " '1oz',\n",
       " '1p',\n",
       " '1paisley',\n",
       " '1password',\n",
       " '1perfectbeauty',\n",
       " '1planet1people',\n",
       " '1pm',\n",
       " '1poundchallenge',\n",
       " '1prdpgn',\n",
       " '1prettyremy',\n",
       " '1ptilf',\n",
       " '1q1b',\n",
       " '1qrf',\n",
       " '1qtpie',\n",
       " '1queenofrock',\n",
       " '1queer1',\n",
       " '1raymo',\n",
       " '1rc',\n",
       " '1realdm623',\n",
       " '1reddiamond',\n",
       " '1republic',\n",
       " '1rick',\n",
       " '1root',\n",
       " '1roxstar',\n",
       " '1runited',\n",
       " '1rzhaner1',\n",
       " '1s',\n",
       " '1sabeau',\n",
       " '1saspursfan',\n",
       " '1scpfg',\n",
       " '1shan',\n",
       " '1shawnl',\n",
       " '1sideofnamprsnd',\n",
       " '1sikgti',\n",
       " '1sixteen6',\n",
       " '1song15',\n",
       " '1soul2laces',\n",
       " '1sr0u',\n",
       " '1st',\n",
       " '1st_lady',\n",
       " '1st_place',\n",
       " '1st_time_caller',\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "temp = vect.fit_transform(x)\n",
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99989, 105849)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 105545)\n"
     ]
    }
   ],
   "source": [
    "# stop_words='english' 英语停用词\n",
    "vect = CountVectorizer(stop_words='english')\n",
    "temp = vect.fit_transform(x)\n",
    "print(temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 90341)\t1\n",
      "  (0, 21637)\t1\n",
      "  (0, 65401)\t1\n",
      "  (1, 79327)\t1\n",
      "  (1, 81468)\t1\n",
      "  (1, 79881)\t1\n",
      "  (1, 98790)\t1\n",
      "  (2, 83053)\t1\n",
      "  (2, 1718)\t1\n",
      "  (3, 83054)\t1\n",
      "  (3, 70791)\t2\n",
      "  (3, 93726)\t1\n",
      "  (3, 67751)\t1\n",
      "  (3, 101111)\t1\n",
      "  (3, 59486)\t1\n",
      "  (3, 279)\t1\n",
      "  (3, 95799)\t1\n",
      "  (3, 73391)\t1\n",
      "  (3, 57602)\t1\n",
      "  (3, 1754)\t1\n",
      "  (4, 97636)\t1\n",
      "  (4, 78881)\t1\n",
      "  (4, 33550)\t1\n",
      "  (4, 48290)\t1\n",
      "  (4, 96326)\t1\n",
      "  :\t:\n",
      "  (99984, 69721)\t1\n",
      "  (99984, 86633)\t1\n",
      "  (99984, 7019)\t1\n",
      "  (99984, 88827)\t1\n",
      "  (99984, 58222)\t1\n",
      "  (99985, 98019)\t1\n",
      "  (99985, 76192)\t1\n",
      "  (99985, 68897)\t1\n",
      "  (99985, 99594)\t1\n",
      "  (99985, 59924)\t1\n",
      "  (99985, 88853)\t1\n",
      "  (99985, 61372)\t1\n",
      "  (99985, 58224)\t1\n",
      "  (99985, 23185)\t1\n",
      "  (99985, 73694)\t1\n",
      "  (99986, 97748)\t1\n",
      "  (99986, 104183)\t1\n",
      "  (99986, 58223)\t1\n",
      "  (99987, 65633)\t1\n",
      "  (99987, 104506)\t2\n",
      "  (99987, 66652)\t1\n",
      "  (99987, 58225)\t1\n",
      "  (99988, 68001)\t1\n",
      "  (99988, 104506)\t1\n",
      "  (99988, 58226)\t1\n"
     ]
    }
   ],
   "source": [
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({'a',\n",
       "           'about',\n",
       "           'above',\n",
       "           'across',\n",
       "           'after',\n",
       "           'afterwards',\n",
       "           'again',\n",
       "           'against',\n",
       "           'all',\n",
       "           'almost',\n",
       "           'alone',\n",
       "           'along',\n",
       "           'already',\n",
       "           'also',\n",
       "           'although',\n",
       "           'always',\n",
       "           'am',\n",
       "           'among',\n",
       "           'amongst',\n",
       "           'amoungst',\n",
       "           'amount',\n",
       "           'an',\n",
       "           'and',\n",
       "           'another',\n",
       "           'any',\n",
       "           'anyhow',\n",
       "           'anyone',\n",
       "           'anything',\n",
       "           'anyway',\n",
       "           'anywhere',\n",
       "           'are',\n",
       "           'around',\n",
       "           'as',\n",
       "           'at',\n",
       "           'back',\n",
       "           'be',\n",
       "           'became',\n",
       "           'because',\n",
       "           'become',\n",
       "           'becomes',\n",
       "           'becoming',\n",
       "           'been',\n",
       "           'before',\n",
       "           'beforehand',\n",
       "           'behind',\n",
       "           'being',\n",
       "           'below',\n",
       "           'beside',\n",
       "           'besides',\n",
       "           'between',\n",
       "           'beyond',\n",
       "           'bill',\n",
       "           'both',\n",
       "           'bottom',\n",
       "           'but',\n",
       "           'by',\n",
       "           'call',\n",
       "           'can',\n",
       "           'cannot',\n",
       "           'cant',\n",
       "           'co',\n",
       "           'con',\n",
       "           'could',\n",
       "           'couldnt',\n",
       "           'cry',\n",
       "           'de',\n",
       "           'describe',\n",
       "           'detail',\n",
       "           'do',\n",
       "           'done',\n",
       "           'down',\n",
       "           'due',\n",
       "           'during',\n",
       "           'each',\n",
       "           'eg',\n",
       "           'eight',\n",
       "           'either',\n",
       "           'eleven',\n",
       "           'else',\n",
       "           'elsewhere',\n",
       "           'empty',\n",
       "           'enough',\n",
       "           'etc',\n",
       "           'even',\n",
       "           'ever',\n",
       "           'every',\n",
       "           'everyone',\n",
       "           'everything',\n",
       "           'everywhere',\n",
       "           'except',\n",
       "           'few',\n",
       "           'fifteen',\n",
       "           'fifty',\n",
       "           'fill',\n",
       "           'find',\n",
       "           'fire',\n",
       "           'first',\n",
       "           'five',\n",
       "           'for',\n",
       "           'former',\n",
       "           'formerly',\n",
       "           'forty',\n",
       "           'found',\n",
       "           'four',\n",
       "           'from',\n",
       "           'front',\n",
       "           'full',\n",
       "           'further',\n",
       "           'get',\n",
       "           'give',\n",
       "           'go',\n",
       "           'had',\n",
       "           'has',\n",
       "           'hasnt',\n",
       "           'have',\n",
       "           'he',\n",
       "           'hence',\n",
       "           'her',\n",
       "           'here',\n",
       "           'hereafter',\n",
       "           'hereby',\n",
       "           'herein',\n",
       "           'hereupon',\n",
       "           'hers',\n",
       "           'herself',\n",
       "           'him',\n",
       "           'himself',\n",
       "           'his',\n",
       "           'how',\n",
       "           'however',\n",
       "           'hundred',\n",
       "           'i',\n",
       "           'ie',\n",
       "           'if',\n",
       "           'in',\n",
       "           'inc',\n",
       "           'indeed',\n",
       "           'interest',\n",
       "           'into',\n",
       "           'is',\n",
       "           'it',\n",
       "           'its',\n",
       "           'itself',\n",
       "           'keep',\n",
       "           'last',\n",
       "           'latter',\n",
       "           'latterly',\n",
       "           'least',\n",
       "           'less',\n",
       "           'ltd',\n",
       "           'made',\n",
       "           'many',\n",
       "           'may',\n",
       "           'me',\n",
       "           'meanwhile',\n",
       "           'might',\n",
       "           'mill',\n",
       "           'mine',\n",
       "           'more',\n",
       "           'moreover',\n",
       "           'most',\n",
       "           'mostly',\n",
       "           'move',\n",
       "           'much',\n",
       "           'must',\n",
       "           'my',\n",
       "           'myself',\n",
       "           'name',\n",
       "           'namely',\n",
       "           'neither',\n",
       "           'never',\n",
       "           'nevertheless',\n",
       "           'next',\n",
       "           'nine',\n",
       "           'no',\n",
       "           'nobody',\n",
       "           'none',\n",
       "           'noone',\n",
       "           'nor',\n",
       "           'not',\n",
       "           'nothing',\n",
       "           'now',\n",
       "           'nowhere',\n",
       "           'of',\n",
       "           'off',\n",
       "           'often',\n",
       "           'on',\n",
       "           'once',\n",
       "           'one',\n",
       "           'only',\n",
       "           'onto',\n",
       "           'or',\n",
       "           'other',\n",
       "           'others',\n",
       "           'otherwise',\n",
       "           'our',\n",
       "           'ours',\n",
       "           'ourselves',\n",
       "           'out',\n",
       "           'over',\n",
       "           'own',\n",
       "           'part',\n",
       "           'per',\n",
       "           'perhaps',\n",
       "           'please',\n",
       "           'put',\n",
       "           'rather',\n",
       "           're',\n",
       "           'same',\n",
       "           'see',\n",
       "           'seem',\n",
       "           'seemed',\n",
       "           'seeming',\n",
       "           'seems',\n",
       "           'serious',\n",
       "           'several',\n",
       "           'she',\n",
       "           'should',\n",
       "           'show',\n",
       "           'side',\n",
       "           'since',\n",
       "           'sincere',\n",
       "           'six',\n",
       "           'sixty',\n",
       "           'so',\n",
       "           'some',\n",
       "           'somehow',\n",
       "           'someone',\n",
       "           'something',\n",
       "           'sometime',\n",
       "           'sometimes',\n",
       "           'somewhere',\n",
       "           'still',\n",
       "           'such',\n",
       "           'system',\n",
       "           'take',\n",
       "           'ten',\n",
       "           'than',\n",
       "           'that',\n",
       "           'the',\n",
       "           'their',\n",
       "           'them',\n",
       "           'themselves',\n",
       "           'then',\n",
       "           'thence',\n",
       "           'there',\n",
       "           'thereafter',\n",
       "           'thereby',\n",
       "           'therefore',\n",
       "           'therein',\n",
       "           'thereupon',\n",
       "           'these',\n",
       "           'they',\n",
       "           'thick',\n",
       "           'thin',\n",
       "           'third',\n",
       "           'this',\n",
       "           'those',\n",
       "           'though',\n",
       "           'three',\n",
       "           'through',\n",
       "           'throughout',\n",
       "           'thru',\n",
       "           'thus',\n",
       "           'to',\n",
       "           'together',\n",
       "           'too',\n",
       "           'top',\n",
       "           'toward',\n",
       "           'towards',\n",
       "           'twelve',\n",
       "           'twenty',\n",
       "           'two',\n",
       "           'un',\n",
       "           'under',\n",
       "           'until',\n",
       "           'up',\n",
       "           'upon',\n",
       "           'us',\n",
       "           'very',\n",
       "           'via',\n",
       "           'was',\n",
       "           'we',\n",
       "           'well',\n",
       "           'were',\n",
       "           'what',\n",
       "           'whatever',\n",
       "           'when',\n",
       "           'whence',\n",
       "           'whenever',\n",
       "           'where',\n",
       "           'whereafter',\n",
       "           'whereas',\n",
       "           'whereby',\n",
       "           'wherein',\n",
       "           'whereupon',\n",
       "           'wherever',\n",
       "           'whether',\n",
       "           'which',\n",
       "           'while',\n",
       "           'whither',\n",
       "           'who',\n",
       "           'whoever',\n",
       "           'whole',\n",
       "           'whom',\n",
       "           'whose',\n",
       "           'why',\n",
       "           'will',\n",
       "           'with',\n",
       "           'within',\n",
       "           'without',\n",
       "           'would',\n",
       "           'yet',\n",
       "           'you',\n",
       "           'your',\n",
       "           'yours',\n",
       "           'yourself',\n",
       "           'yourselves'})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 31)\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df = .05)\n",
    "temp = vect.fit_transform(x)\n",
    "print(temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 3219557)\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,5))\n",
    "temp = vect.fit_transform(x)\n",
    "print(temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '00 01',\n",
       " '00 01 minute',\n",
       " '00 01 minute after',\n",
       " '00 01 minute after applications',\n",
       " '00 10',\n",
       " '00 10 00',\n",
       " '00 56',\n",
       " '00 56 03',\n",
       " '00 56 03 beating',\n",
       " '00 56 03 beating my',\n",
       " '00 always',\n",
       " '00 always have',\n",
       " '00 always have such',\n",
       " '00 always have such warmth']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names()[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('hello') # 词干"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'interest'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('interesting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'interest'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('interested')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             is so sad for my APL frie...\n",
       "1                           I missed the New Moon trail...\n",
       "2                                  omg its already 7:30 :O\n",
       "3                  .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4                 i think mi bf is cheating on me!!!   ...\n",
       "                               ...                        \n",
       "99984    @Cupcake  seems like a repeating problem   hop...\n",
       "99985    @cupcake__ arrrr we both replied to each other...\n",
       "99986                       @CuPcAkE_2120 ya i thought so \n",
       "99987    @Cupcake_Dollie Yes. Yes. I'm glad you had mor...\n",
       "99988                      @cupcake_kayla haha yes you do \n",
       "Name: SentimentText, Length: 99989, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 65503)\t0.39300895185119816\n",
      "  (0, 21665)\t0.7454662730740867\n",
      "  (0, 80817)\t0.2007897391947143\n",
      "  (0, 65062)\t0.2049763574474265\n",
      "  (0, 90542)\t0.3371941933510279\n",
      "  (0, 93674)\t0.22523369266874163\n",
      "  (0, 71938)\t0.20751393363509252\n"
     ]
    }
   ],
   "source": [
    "vect = TfidfVectorizer()\n",
    "_ = vect.fit_transform(x)\n",
    "print(_[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 71938)\t1\n",
      "  (0, 93674)\t1\n",
      "  (0, 90542)\t1\n",
      "  (0, 65062)\t1\n",
      "  (0, 80817)\t1\n",
      "  (0, 21665)\t1\n",
      "  (0, 65503)\t1\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "_ = vect.fit_transform(x)\n",
    "print(_[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.564632\n",
       "0    0.435368\n",
       "Name: Sentiment, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7557431394487152 {'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__stop_words': None}\n"
     ]
    }
   ],
   "source": [
    "pipe_params = {\n",
    "    \"vect__ngram_range\":[(1,1),(1,2)],\n",
    "    \"vect__max_features\":[1000,10000],\n",
    "    \"vect__stop_words\":[None,'english']\n",
    "}\n",
    "pipe = Pipeline([('vect',CountVectorizer()),('classifier',MultinomialNB())])\n",
    "grid = GridSearchCV(pipe,pipe_params,cv = 5)\n",
    "grid.fit(x,y)\n",
    "print(grid.best_score_,grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7601336085001209 {'tfid__max_features': 10000, 'tfid__ngram_range': (1, 2), 'tfid__stop_words': None}\n"
     ]
    }
   ],
   "source": [
    "pipe_params = {\n",
    "    \"tfid__ngram_range\":[(1,1),(1,2)],\n",
    "    \"tfid__max_features\":[1000,10000],\n",
    "    \"tfid__stop_words\":[None,'english']\n",
    "}\n",
    "pipe = Pipeline([('tfid',TfidfVectorizer()),('classifier',MultinomialNB())])\n",
    "grid = GridSearchCV(pipe,pipe_params,cv = 5)\n",
    "grid.fit(x,y)\n",
    "print(grid.best_score_,grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureizer = FeatureUnion(\n",
    "    [(\"tfid_vect\",TfidfVectorizer()),\n",
    "     (\"count_vect\",CountVectorizer())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 211698)\n"
     ]
    }
   ],
   "source": [
    "_ = featureizer.fit_transform(x)\n",
    "print(_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featureizer.set_params(trid_vect__max_features = 100,\n",
    "#                        count_vect__ngram_range(1,2),\n",
    "#                        count_vect__max_features = 300)\n",
    "# _ = featurizer.fit_transform(x)\n",
    "# print(_,shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7582334159804939 {'featureizer__count_vect__max_features': 10000, 'featureizer__count_vect__ngram_range': (1, 2), 'featureizer__count_vect__stop_words': None, 'featureizer__tfid_vect__max_features': 10000, 'featureizer__tfid_vect__ngram_range': (1, 1), 'featureizer__tfid_vect__stop_words': 'english'}\n",
      "-------------------------------------------\n",
      "运行时间: 1686.7748041152954\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "pipe_params = {\n",
    "    \"featureizer__count_vect__ngram_range\":[(1,1),(1,2)],\n",
    "    \"featureizer__count_vect__max_features\":[1000,10000],\n",
    "    \"featureizer__count_vect__stop_words\":[None,'english'],\n",
    "    \"featureizer__tfid_vect__ngram_range\":[(1,1),(1,2)],\n",
    "    \"featureizer__tfid_vect__max_features\":[1000,10000],\n",
    "    \"featureizer__tfid_vect__stop_words\":[None,'english']}\n",
    "pipe = Pipeline(\n",
    "    [('featureizer',featureizer),\n",
    "     ('classifier',MultinomialNB())])\n",
    "grid = GridSearchCV(pipe,pipe_params,cv = 5)\n",
    "grid.fit(x,y)\n",
    "print(grid.best_score_,grid.best_params_)\n",
    "end = time.time()\n",
    "print(\"-------------------------------------------\")\n",
    "print(f\"运行时间: {end - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
